{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0-ml0cgl82-1b13sp",
      "metadata": {},
      "source": [
        "# Alternating Layer Freezing: A GAN-Inspired Training Strategy\n",
        "\n",
        "This notebook explores an experimental training strategy inspired by GANs: freezing half of a neural network's layers while training the other half, then alternating. We'll compare this approach against standard training to evaluate:\n",
        "\n",
        "1. Training speed (convergence rate)\n",
        "2. Final model performance (accuracy)\n",
        "\n",
        "We'll use a simple feedforward neural network on the MNIST dataset and implement three training strategies:\n",
        "\n",
        "- Standard training (all layers updated every epoch)\n",
        "- Alternating freeze (first half vs second half of layers)\n",
        "- Per-epoch alternating (switch every epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-1-ml0cgl83-n5uee4",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-2-ml0cgl83-xu4jfx",
      "metadata": {},
      "source": [
        "## Load and Prepare MNIST Dataset\n",
        "\n",
        "We'll use a subset of MNIST for faster experimentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-3-ml0cgl83-yylovb",
      "metadata": {},
      "source": [
        "# Load MNIST dataset\n",
        "print('Loading MNIST dataset...')\n",
        "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "X, y = mnist['data'], mnist['target'].astype(int)\n",
        "\n",
        "# Use subset for faster training\n",
        "X = np.array(X[:10000])\n",
        "y = np.array(y[:10000])\n",
        "\n",
        "# Normalize\n",
        "X = X / 255.0\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.LongTensor(y_train)\n",
        "X_test_t = torch.FloatTensor(X_test)\n",
        "y_test_t = torch.LongTensor(y_test)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(f'Training samples: {len(X_train)}')\n",
        "print(f'Test samples: {len(X_test)}')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-4-ml0cgl83-v1c70f",
      "metadata": {},
      "source": [
        "## Define Neural Network Architecture\n",
        "\n",
        "We'll create a simple 4-layer feedforward network that can be split in half for the alternating freeze strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-5-ml0cgl83-ro59o5",
      "metadata": {},
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size=256, num_classes=10):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # First half of the network\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        # Second half of the network\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "        # Group layers for easy freezing\n",
        "        self.first_half = [self.fc1, self.fc2]\n",
        "        self.second_half = [self.fc3, self.fc4]\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.relu3(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "    \n",
        "    def freeze_first_half(self):\n",
        "        for layer in self.first_half:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "    \n",
        "    def freeze_second_half(self):\n",
        "        for layer in self.second_half:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "    \n",
        "    def unfreeze_all(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-6-ml0cgl83-tcdyib",
      "metadata": {},
      "source": [
        "## Training Functions\n",
        "\n",
        "We'll implement different training strategies and a common evaluation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-7-ml0cgl83-ujzjn5",
      "metadata": {},
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"Evaluate model accuracy on test set\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "def train_standard(model, train_loader, test_loader, epochs=20, lr=0.001):\n",
        "    \"\"\"Standard training: update all layers every epoch\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_acc = evaluate_model(model, test_loader)\n",
        "        test_accuracies.append(test_acc)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    return train_losses, test_accuracies, training_time\n",
        "\n",
        "\n",
        "def train_alternating_freeze(model, train_loader, test_loader, epochs=20, lr=0.001):\n",
        "    \"\"\"Alternating freeze: switch between first and second half every epoch\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        model.unfreeze_all()\n",
        "        \n",
        "        # Alternate which half to freeze\n",
        "        if epoch % 2 == 0:\n",
        "            model.freeze_second_half()  # Train first half\n",
        "        else:\n",
        "            model.freeze_first_half()   # Train second half\n",
        "        \n",
        "        epoch_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_acc = evaluate_model(model, test_loader)\n",
        "        test_accuracies.append(test_acc)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    return train_losses, test_accuracies, training_time\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-8-ml0cgl83-64w9x1",
      "metadata": {},
      "source": [
        "## Experiment 1: Standard Training\n",
        "\n",
        "Train a model using the standard approach where all layers are updated every epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-9-ml0cgl83-956yj5",
      "metadata": {},
      "source": [
        "print('Training with STANDARD approach...')\n",
        "model_standard = SimpleNN()\n",
        "losses_std, accs_std, time_std = train_standard(model_standard, train_loader, test_loader, epochs=20)\n",
        "print(f'Training time: {time_std:.2f}s')\n",
        "print(f'Final test accuracy: {accs_std[-1]:.2f}%')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-10-ml0cgl83-0ytvzw",
      "metadata": {},
      "source": [
        "## Experiment 2: Alternating Freeze Training\n",
        "\n",
        "Train a model using the alternating freeze strategy, switching between first and second half of layers each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-11-ml0cgl83-tg6m7v",
      "metadata": {},
      "source": [
        "print('Training with ALTERNATING FREEZE approach...')\n",
        "model_alternating = SimpleNN()\n",
        "losses_alt, accs_alt, time_alt = train_alternating_freeze(model_alternating, train_loader, test_loader, epochs=20)\n",
        "print(f'Training time: {time_alt:.2f}s')\n",
        "print(f'Final test accuracy: {accs_alt[-1]:.2f}%')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-12-ml0cgl83-ye2c54",
      "metadata": {},
      "source": [
        "## Results Comparison\n",
        "\n",
        "Let's visualize the training dynamics and compare the two approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-13-ml0cgl83-4kapg3",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot training loss\n",
        "axes[0].plot(losses_std, label='Standard', marker='o', linewidth=2)\n",
        "axes[0].plot(losses_alt, label='Alternating Freeze', marker='s', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Training Loss', fontsize=12)\n",
        "axes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot test accuracy\n",
        "axes[1].plot(accs_std, label='Standard', marker='o', linewidth=2)\n",
        "axes[1].plot(accs_alt, label='Alternating Freeze', marker='s', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-14-ml0cgl83-etlb18",
      "metadata": {},
      "source": [
        "## Quantitative Analysis\n",
        "\n",
        "Let's compute key metrics to answer our research questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-15-ml0cgl83-x1m6mg",
      "metadata": {},
      "source": [
        "print('=' * 60)\n",
        "print('PERFORMANCE COMPARISON')\n",
        "print('=' * 60)\n",
        "\n",
        "print('\\n1. TRAINING SPEED:')\n",
        "print(f'   Standard training time: {time_std:.2f}s')\n",
        "print(f'   Alternating freeze time: {time_alt:.2f}s')\n",
        "speedup = ((time_std - time_alt) / time_std) * 100\n",
        "print(f'   Speed difference: {speedup:+.1f}%')\n",
        "\n",
        "print('\\n2. FINAL PERFORMANCE:')\n",
        "print(f'   Standard accuracy: {accs_std[-1]:.2f}%')\n",
        "print(f'   Alternating freeze accuracy: {accs_alt[-1]:.2f}%')\n",
        "acc_diff = accs_alt[-1] - accs_std[-1]\n",
        "print(f'   Accuracy difference: {acc_diff:+.2f}%')\n",
        "\n",
        "print('\\n3. CONVERGENCE SPEED:')\n",
        "# Find epoch where 90% of final accuracy is reached\n",
        "threshold_std = 0.9 * accs_std[-1]\n",
        "threshold_alt = 0.9 * accs_alt[-1]\n",
        "epoch_std = next((i for i, acc in enumerate(accs_std) if acc >= threshold_std), len(accs_std))\n",
        "epoch_alt = next((i for i, acc in enumerate(accs_alt) if acc >= threshold_alt), len(accs_alt))\n",
        "print(f'   Standard reaches 90% final accuracy at epoch: {epoch_std}')\n",
        "print(f'   Alternating freeze reaches 90% final accuracy at epoch: {epoch_alt}')\n",
        "\n",
        "print('\\n4. TRAINING STABILITY:')\n",
        "std_loss_var = np.std(losses_std)\n",
        "alt_loss_var = np.std(losses_alt)\n",
        "print(f'   Standard loss std dev: {std_loss_var:.4f}')\n",
        "print(f'   Alternating freeze loss std dev: {alt_loss_var:.4f}')\n",
        "print('=' * 60)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-16-ml0cgl83-6rzfs1",
      "metadata": {},
      "source": [
        "## Conclusion and Insights\n",
        "\n",
        "Based on our experiments, we can draw the following conclusions about the alternating freeze training strategy:\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. Training Speed: The alternating freeze approach may show slight variations in training time, but typically doesn't provide significant speedup. The backward pass still needs to compute gradients through the entire network.\n",
        "2. Performance: The alternating freeze strategy generally achieves comparable or slightly lower final accuracy compared to standard training. This is because only half the network is being optimized at each step.\n",
        "3. Convergence: Standard training typically converges more smoothly and reliably, while alternating freeze can show oscillations as different parts of the network are updated.\n",
        "\n",
        "### Why This Differs from GANs\n",
        "\n",
        "The alternating freeze strategy works well for GANs because:\n",
        "\n",
        "- The generator and discriminator have opposing objectives (adversarial)\n",
        "- Training both simultaneously can lead to instability\n",
        "- Each network needs to adapt to the other's current state\n",
        "\n",
        "In contrast, standard feedforward networks have all layers working cooperatively toward the same objective, so freezing layers can hinder the coordinated optimization needed for best performance.\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "For standard supervised learning tasks, stick with traditional training where all layers are updated together. The alternating freeze strategy is best reserved for adversarial or multi-objective scenarios where different network components have conflicting goals.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Alternating Layer Freezing: A GAN-Inspired Training Strategy\n",
    "\n",
    "## Can GAN-style alternating optimization improve standard neural network training?\n",
    "\n",
    "---\n",
    "\n",
    "In **Generative Adversarial Networks (GANs)**, the generator and discriminator are trained alternately â€” freezing one while updating the other. This prevents one network from dominating and helps with training stability.\n",
    "\n",
    "**This notebook explores:** What if we apply the same principle to *standard* neural networks? Could freezing half the layers while training the other half improve:\n",
    "\n",
    "- âš¡ **Training speed** (fewer computations per step)\n",
    "- ðŸŽ¯ **Final accuracy** (better generalization)\n",
    "- ðŸ“ˆ **Convergence** (faster to reach good performance)\n",
    "\n",
    "### Training Strategies Compared\n",
    "\n",
    "| Strategy | Description |\n",
    "|----------|-------------|\n",
    "| **Standard** | All layers updated every step |\n",
    "| **Alternating (Epoch)** | Switch which half is trained each epoch |\n",
    "| **Alternating (Batch)** | Switch which half is trained each batch |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'ðŸ–¥ï¸  Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Load CIFAR-10 Dataset\n",
    "\n",
    "We'll use CIFAR-10 â€” a challenging real-world benchmark with 60,000 32x32 color images across 10 classes (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, trucks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print('ðŸ“¥ Loading CIFAR-10 dataset...')\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'âœ“ Training samples: {len(train_dataset):,}')\n",
    "print(f'âœ“ Test samples: {len(test_dataset):,}')\n",
    "print(f'âœ“ Classes: {train_dataset.classes}')\n",
    "print(f'âœ“ Batches per epoch: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ‘€ Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images\n",
    "fig, axes = plt.subplots(2, 8, figsize=(14, 4), facecolor='#0f0f1a')\n",
    "fig.suptitle('Sample CIFAR-10 Images', fontsize=14, fontweight='bold', color='white')\n",
    "\n",
    "# Denormalize for display\n",
    "mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "std = np.array([0.2470, 0.2435, 0.2616])\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = train_dataset[i]\n",
    "    img = img.numpy().transpose((1, 2, 0))  # CHW -> HWC\n",
    "    img = std * img + mean  # Denormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(train_dataset.classes[label], fontsize=9, color='white')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Neural Network Architecture\n",
    "\n",
    "We create a feedforward network that can be **split in half** for alternating training:\n",
    "\n",
    "```\n",
    "Input (3072) â†’ [FIRST HALF: FC1 + BN + ReLU] â†’ [SECOND HALF: FC2 + BN + ReLU + FC3] â†’ Output (10)\n",
    "```\n",
    "\n",
    "CIFAR-10 images are 32Ã—32Ã—3 = 3072 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Neural network with freeze/unfreeze capabilities for each half.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=3072, hidden_size=512, num_classes=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # FIRST HALF of the network\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # SECOND HALF of the network\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten: (B, 3, 32, 32) -> (B, 3072)\n",
    "        # First half\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        # Second half\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def freeze_first_half(self):\n",
    "        \"\"\"Freeze first half: fc1, bn1\"\"\"\n",
    "        for param in self.fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bn1.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def freeze_second_half(self):\n",
    "        \"\"\"Freeze second half: fc2, bn2, fc3\"\"\"\n",
    "        for param in self.fc2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bn2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.fc3.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_first_half(self):\n",
    "        for param in self.fc1.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.bn1.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze_second_half(self):\n",
    "        for param in self.fc2.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.bn2.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.fc3.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        self.unfreeze_first_half()\n",
    "        self.unfreeze_second_half()\n",
    "\n",
    "# Test model\n",
    "model = SimpleNN()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'ðŸ“Š Model Parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Training Functions\n",
    "\n",
    "We implement three training strategies with the same hyperparameters for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device='cpu'):\n",
    "    \"\"\"Evaluate model accuracy on test set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def train_standard(model, train_loader, test_loader, epochs=15, lr=0.001, device='cpu'):\n",
    "    \"\"\"Standard training: update all layers every step.\"\"\"\n",
    "    model.to(device)\n",
    "    model.unfreeze_all()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses, accuracies = [], []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc='Standard Training'):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        accuracies.append(evaluate(model, test_loader, device))\n",
    "    \n",
    "    return losses, accuracies, time.time() - start\n",
    "\n",
    "\n",
    "def train_alternating_epoch(model, train_loader, test_loader, epochs=15, lr=0.001, device='cpu'):\n",
    "    \"\"\"Alternating training: switch which half is frozen each EPOCH.\"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses, accuracies = [], []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc='Alternating (Epoch)'):\n",
    "        model.train()\n",
    "        model.unfreeze_all()\n",
    "        \n",
    "        # Alternate which half to freeze\n",
    "        if epoch % 2 == 0:\n",
    "            model.freeze_second_half()  # Train first half\n",
    "        else:\n",
    "            model.freeze_first_half()   # Train second half\n",
    "        \n",
    "        # Recreate optimizer with only trainable params\n",
    "        trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = optim.Adam(trainable, lr=lr)\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        accuracies.append(evaluate(model, test_loader, device))\n",
    "    \n",
    "    model.unfreeze_all()\n",
    "    return losses, accuracies, time.time() - start\n",
    "\n",
    "\n",
    "def train_alternating_batch(model, train_loader, test_loader, epochs=15, lr=0.001, device='cpu'):\n",
    "    \"\"\"Alternating training: switch which half is frozen each BATCH.\"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses, accuracies = [], []\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc='Alternating (Batch)'):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Alternate at each batch\n",
    "            model.unfreeze_all()\n",
    "            if batch_idx % 2 == 0:\n",
    "                model.freeze_second_half()\n",
    "            else:\n",
    "                model.freeze_first_half()\n",
    "            \n",
    "            trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = optim.Adam(trainable, lr=lr)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        accuracies.append(evaluate(model, test_loader, device))\n",
    "    \n",
    "    model.unfreeze_all()\n",
    "    return losses, accuracies, time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Run Experiments\n",
    "\n",
    "Let's train three models with identical architecture but different training strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print('='*60)\n",
    "print('ðŸ”¬ EXPERIMENT 1: STANDARD TRAINING')\n",
    "print('='*60)\n",
    "model_std = SimpleNN().to(device)\n",
    "losses_std, accs_std, time_std = train_standard(\n",
    "    model_std, train_loader, test_loader, epochs=EPOCHS, lr=LEARNING_RATE, device=device\n",
    ")\n",
    "results['Standard'] = {'losses': losses_std, 'accuracies': accs_std, 'time': time_std}\n",
    "print(f'âœ“ Final Accuracy: {accs_std[-1]:.2f}% | Time: {time_std:.2f}s')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('ðŸ”¬ EXPERIMENT 2: ALTERNATING (EPOCH-LEVEL)')\n",
    "print('='*60)\n",
    "model_alt_epoch = SimpleNN().to(device)\n",
    "losses_alt_e, accs_alt_e, time_alt_e = train_alternating_epoch(\n",
    "    model_alt_epoch, train_loader, test_loader, epochs=EPOCHS, lr=LEARNING_RATE, device=device\n",
    ")\n",
    "results['Alternating (Epoch)'] = {'losses': losses_alt_e, 'accuracies': accs_alt_e, 'time': time_alt_e}\n",
    "print(f'âœ“ Final Accuracy: {accs_alt_e[-1]:.2f}% | Time: {time_alt_e:.2f}s')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('ðŸ”¬ EXPERIMENT 3: ALTERNATING (BATCH-LEVEL)')\n",
    "print('='*60)\n",
    "model_alt_batch = SimpleNN().to(device)\n",
    "losses_alt_b, accs_alt_b, time_alt_b = train_alternating_batch(\n",
    "    model_alt_batch, train_loader, test_loader, epochs=EPOCHS, lr=LEARNING_RATE, device=device\n",
    ")\n",
    "results['Alternating (Batch)'] = {'losses': losses_alt_b, 'accuracies': accs_alt_b, 'time': time_alt_b}\n",
    "print(f'âœ“ Final Accuracy: {accs_alt_b[-1]:.2f}% | Time: {time_alt_b:.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Visualization: Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors\n",
    "colors = {\n",
    "    'Standard': '#2E86AB',\n",
    "    'Alternating (Epoch)': '#A23B72',\n",
    "    'Alternating (Batch)': '#F18F01'\n",
    "}\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), facecolor='#0f0f1a')\n",
    "fig.suptitle('GAN-Inspired Alternating Layer Freezing: CIFAR-10 Results', \n",
    "             fontsize=16, fontweight='bold', color='white', y=1.02)\n",
    "\n",
    "# Style function\n",
    "def style_axis(ax, title):\n",
    "    ax.set_facecolor('#1a1a2e')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold', color='white', pad=10)\n",
    "    ax.tick_params(colors='white')\n",
    "    ax.grid(True, alpha=0.2, color='#404080')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('#404080')\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "ax1 = axes[0, 0]\n",
    "for name, data in results.items():\n",
    "    ax1.plot(range(1, EPOCHS+1), data['losses'], label=name, color=colors[name], \n",
    "             linewidth=2.5, marker='o', markersize=5)\n",
    "ax1.set_xlabel('Epoch', color='white')\n",
    "ax1.set_ylabel('Training Loss', color='white')\n",
    "ax1.legend(facecolor='#2a2a4a', edgecolor='#404080', labelcolor='white')\n",
    "style_axis(ax1, 'Training Loss Over Time')\n",
    "\n",
    "# Plot 2: Test Accuracy\n",
    "ax2 = axes[0, 1]\n",
    "for name, data in results.items():\n",
    "    ax2.plot(range(1, EPOCHS+1), data['accuracies'], label=name, color=colors[name], \n",
    "             linewidth=2.5, marker='s', markersize=5)\n",
    "ax2.set_xlabel('Epoch', color='white')\n",
    "ax2.set_ylabel('Test Accuracy (%)', color='white')\n",
    "ax2.legend(facecolor='#2a2a4a', edgecolor='#404080', labelcolor='white')\n",
    "style_axis(ax2, 'Test Accuracy Over Time')\n",
    "\n",
    "# Plot 3: Training Time\n",
    "ax3 = axes[1, 0]\n",
    "names = list(results.keys())\n",
    "times = [results[n]['time'] for n in names]\n",
    "bars = ax3.barh(names, times, color=[colors[n] for n in names], edgecolor='white', height=0.5)\n",
    "for bar, t in zip(bars, times):\n",
    "    ax3.text(t + 1, bar.get_y() + bar.get_height()/2, f'{t:.1f}s', \n",
    "             va='center', color='white', fontweight='bold')\n",
    "ax3.set_xlabel('Time (seconds)', color='white')\n",
    "style_axis(ax3, 'Total Training Time')\n",
    "\n",
    "# Plot 4: Final Accuracy\n",
    "ax4 = axes[1, 1]\n",
    "final_accs = [results[n]['accuracies'][-1] for n in names]\n",
    "bars = ax4.barh(names, final_accs, color=[colors[n] for n in names], edgecolor='white', height=0.5)\n",
    "for bar, acc in zip(bars, final_accs):\n",
    "    ax4.text(acc - 3, bar.get_y() + bar.get_height()/2, f'{acc:.1f}%', \n",
    "             va='center', color='white', fontweight='bold')\n",
    "ax4.set_xlabel('Accuracy (%)', color='white')\n",
    "ax4.set_xlim(min(final_accs) - 5, max(final_accs) + 5)\n",
    "style_axis(ax4, 'Final Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_results.png', dpi=150, facecolor='#0f0f1a', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('                    ðŸ“Š PERFORMANCE COMPARISON')\n",
    "print('='*70)\n",
    "\n",
    "std_acc = results['Standard']['accuracies'][-1]\n",
    "std_time = results['Standard']['time']\n",
    "\n",
    "print(f\"\\n{'Strategy':<25} {'Accuracy':<15} {'Time':<15} {'vs Standard'}\")\n",
    "print('-'*70)\n",
    "\n",
    "for name, data in results.items():\n",
    "    acc = data['accuracies'][-1]\n",
    "    t = data['time']\n",
    "    acc_diff = acc - std_acc\n",
    "    time_ratio = t / std_time\n",
    "    \n",
    "    if name == 'Standard':\n",
    "        print(f\"{name:<25} {acc:.2f}%{'':8} {t:.2f}s{'':8} (baseline)\")\n",
    "    else:\n",
    "        print(f\"{name:<25} {acc:.2f}%{'':8} {t:.2f}s{'':8} {acc_diff:+.2f}% acc, {time_ratio:.2f}x time\")\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('                         ðŸ” KEY INSIGHTS')\n",
    "print('='*70)\n",
    "print(\"\"\"\n",
    "1. CIFAR-10 IS HARDER: Unlike MNIST's ~97% accuracy, CIFAR-10 typically\n",
    "   achieves 50-60% with simple MLPs (CNNs get 90%+).\n",
    "\n",
    "2. TRAINING SPEED: Alternating approaches have similar training time\n",
    "   because gradient computation still traverses the full network.\n",
    "\n",
    "3. FINAL ACCURACY: Standard training typically achieves best accuracy\n",
    "   because all layers cooperate toward the same objective.\n",
    "\n",
    "4. WHY THIS DIFFERS FROM GANs:\n",
    "   - GANs have ADVERSARIAL objectives (G vs D)\n",
    "   - Standard NNs have COOPERATIVE layers (all minimize same loss)\n",
    "   - Alternating helps stability in adversarial setups, not cooperative ones\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Conclusion\n",
    "\n",
    "### Key Findings on CIFAR-10\n",
    "\n",
    "| Metric | Standard | Alternating (Epoch) | Alternating (Batch) |\n",
    "|--------|----------|--------------------|--------------------|  \n",
    "| Accuracy | Best | Similar/Lower | Lower |\n",
    "| Speed | Baseline | ~Similar | Similar |\n",
    "| Stability | High | Medium | Low |\n",
    "\n",
    "### Why GAN-Style Training Doesn't Help Standard Networks\n",
    "\n",
    "In **GANs**, alternating training works because:\n",
    "- Generator and Discriminator have **opposing goals**\n",
    "- Training both simultaneously causes instability\n",
    "- Each needs to adapt to the other's current state\n",
    "\n",
    "In **standard networks**:\n",
    "- All layers have the **same goal** (minimize loss)\n",
    "- Layers need **coordinated updates** for effective learning\n",
    "- Freezing half the network disrupts gradient flow\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "For standard supervised learning, **stick with traditional training** where all layers are updated together. Alternating strategies are best suited for adversarial or multi-objective scenarios.\n",
    "\n",
    "Note: For better CIFAR-10 performance, use **Convolutional Neural Networks (CNNs)** instead of MLPs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

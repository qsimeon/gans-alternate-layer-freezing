# ğŸ”¬ Alternating Layer Freezing: A GAN-Inspired Training Experiment

> **Can GAN-style alternating optimization improve standard neural network training?**

In Generative Adversarial Networks (GANs), the generator and discriminator are trained alternatelyâ€”freezing one while updating the other. This project investigates whether applying the same alternating freeze-and-train approach to different halves of *standard* neural networks can improve training speed or final performance.

![Results](comparison_results.png)

## ğŸ¯ Research Question

**If alternating training helps stabilize GANs, could it also help standard feedforward networks?**

| Training Strategy | Description |
|------------------|-------------|
| **Standard** | All layers updated at every step |
| **Alternating (Epoch)** | Switch which half is trained each epoch |
| **Alternating (Batch)** | Switch which half is trained each batch |

## ğŸ“Š Key Findings

| Metric | Standard | Alternating |
|--------|----------|-------------|
| **Final Accuracy** | âœ… Best | Similar/Slightly Lower |
| **Training Speed** | Baseline | ~Similar |
| **Stability** | High | Medium |

**Conclusion:** Alternating layer freezing works for GANs because they have *adversarial* objectives. Standard networks have *cooperative* layers all minimizing the same lossâ€”freezing half disrupts the coordinated gradient flow needed for optimal learning.

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run the full experiment suite
python experiments.py

# Or explore interactively
jupyter lab notebook.ipynb
```

## ğŸ“ Project Structure

```
gans-alternate-layer-freezing/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ ARCHITECTURE.md        # Detailed system architecture with diagrams
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ models.py              # Neural network with freeze/unfreeze methods
â”œâ”€â”€ training_loop.py       # Standard & alternating training implementations
â”œâ”€â”€ experiments.py         # Main experiment runner with visualizations
â”œâ”€â”€ notebook.ipynb         # Interactive Jupyter notebook
â””â”€â”€ comparison_results.png # Generated visualization
```

## ğŸ—ï¸ Architecture

The network is split into two halves for alternating training:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input (3072)   CIFAR-10 image (32x32x3 flattened)     â”‚
â”‚       â”‚                                                 â”‚
â”‚       â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           FIRST HALF (Frozen in Phase 2)        â”‚   â”‚
â”‚  â”‚  Linear(3072â†’512) â†’ BatchNorm â†’ ReLU â†’ Dropout  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                 â”‚
â”‚       â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚          SECOND HALF (Frozen in Phase 1)        â”‚   â”‚
â”‚  â”‚  Linear(512â†’512) â†’ BatchNorm â†’ ReLU â†’ Dropout   â”‚   â”‚
â”‚  â”‚  Linear(512â†’10)                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                                 â”‚
â”‚       â–¼                                                 â”‚
â”‚  Output (10)    Class probabilities for 10 classes     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ How It Works

### Layer Freezing Mechanism

```python
def freeze_first_half(self):
    for param in self.fc1.parameters():
        param.requires_grad = False  # No gradient updates
```

When `requires_grad=False`:
- Forward pass works normally
- Backward pass skips gradient computation
- Optimizer doesn't update frozen parameters

### Training Loop

```python
# Epoch 1: Train first half, freeze second
model.unfreeze_first_half()
model.freeze_second_half()

# Epoch 2: Train second half, freeze first
model.freeze_first_half()
model.unfreeze_second_half()
```

## ğŸ“ˆ Why GANs â‰  Standard Networks

```
GAN Training:                    Standard Network:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Generator â†â”€ ADVERSARIAL â”€â†’ Discriminator    Layer1 â†’ Layer2 â†’ Layer3 â†’ Output
    â”‚              â”‚                              â”‚         â”‚         â”‚
    â””â”€â”€ Opposing goals (fool vs detect)           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€ Alternating prevents dominance                    â”‚
                                                  Same goal (minimize loss)
                                                  Needs coordinated updates
```

## ğŸ§ª Experiment Details

- **Dataset:** CIFAR-10 (60,000 32x32 color images, 10 classes)
- **Classes:** airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck
- **Architecture:** 3-layer feedforward network (3072 â†’ 512 â†’ 512 â†’ 10)
- **Optimizer:** Adam (lr=0.001)
- **Epochs:** 15
- **Batch Size:** 128

*Note: For production CIFAR-10 tasks, use CNNs which achieve 90%+ accuracy. MLPs are used here to focus on the training strategy comparison.*

## ğŸ“š Learn More

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed system diagrams and implementation notes.

## ğŸ“ License

MIT License - Feel free to use this for learning and experimentation!

---

*This project was created with [Automated Idea Expansion](https://github.com/qsimeon/automated-idea-expansion) to explore whether GAN training principles can improve standard neural networks.*
